{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86c8006-7daf-4053-a85c-4db16d052ba1",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ZuchniakK/CryptoDataProcessing/blob/main/4_data_save.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6794c98d",
   "metadata": {},
   "source": [
    "### In this notebook I put together all dataset generation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2d0a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import zipfile\n",
    "from concurrent.futures import ALL_COMPLETED, ThreadPoolExecutor, wait\n",
    "from datetime import date, datetime, timedelta\n",
    "from functools import partial\n",
    "from os import makedirs\n",
    "from os.path import exists, join\n",
    "from pprint import pprint\n",
    "from time import monotonic\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import talib\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer, StandardScaler\n",
    "\n",
    "# I put the functions defined in previous notebooks in the crypto_data.py file and we will import them from there\n",
    "from crypto_data import (\n",
    "    daterange,\n",
    "    download_extract_zip,\n",
    "    get_trades,\n",
    "    merge_ohlc,\n",
    "    trades2ohlc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09031e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [8, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcf486e",
   "metadata": {},
   "source": [
    "Categorization of features from previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f7bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONOTONIC_COL = [\n",
    "    \"SQZ_NO\",\n",
    "    \"SQZPRO_NO\",\n",
    "    \"TOS_STDEVALL_LR\",\n",
    "    \"TOS_STDEVALL_L_1\",\n",
    "    \"TOS_STDEVALL_U_1\",\n",
    "    \"TOS_STDEVALL_L_2\",\n",
    "    \"TOS_STDEVALL_U_2\",\n",
    "    \"TOS_STDEVALL_L_3\",\n",
    "    \"TOS_STDEVALL_U_3\",\n",
    "]\n",
    "\n",
    "PAIR_COL = [\n",
    "    \"HILOl_13_21\",\n",
    "    \"HILOs_13_21\",\n",
    "    \"PSARl_0.02_0.2\",\n",
    "    \"PSARs_0.02_0.2\",\n",
    "    \"QQEl_14_5_4.236\",\n",
    "    \"QQEs_14_5_4.236\",\n",
    "    \"SUPERTl_7_3.0\",\n",
    "    \"SUPERTs_7_3.0\",\n",
    "]\n",
    "\n",
    "CANDLE_COL = [\n",
    "    \"CDL_RICKSHAWMAN\",\n",
    "    \"CDL_DARKCLOUDCOVER\",\n",
    "    \"CDL_HOMINGPIGEON\",\n",
    "    \"CDL_LONGLEGGEDDOJI\",\n",
    "    \"CDL_MATCHINGLOW\",\n",
    "    \"CDL_SPINNINGTOP\",\n",
    "    \"CDL_ADVANCEBLOCK\",\n",
    "    \"CDL_PIERCING\",\n",
    "    \"CDL_3WHITESOLDIERS\",\n",
    "    \"CDL_HIKKAKE\",\n",
    "    \"CDL_SHOOTINGSTAR\",\n",
    "    \"CDL_3LINESTRIKE\",\n",
    "    \"CDL_STICKSANDWICH\",\n",
    "    \"CDL_CONCEALBABYSWALL\",\n",
    "    \"CDL_DOJISTAR\",\n",
    "    \"CDL_GAPSIDESIDEWHITE\",\n",
    "    \"CDL_KICKINGBYLENGTH\",\n",
    "    \"CDL_HARAMICROSS\",\n",
    "    \"CDL_3INSIDE\",\n",
    "    \"CDL_BREAKAWAY\",\n",
    "    \"CDL_EVENINGDOJISTAR\",\n",
    "    \"CDL_UPSIDEGAP2CROWS\",\n",
    "    \"CDL_XSIDEGAP3METHODS\",\n",
    "    \"CDL_INSIDE\",\n",
    "    \"CDL_ONNECK\",\n",
    "    \"CDL_BELTHOLD\",\n",
    "    \"CDL_MARUBOZU\",\n",
    "    \"CDL_ABANDONEDBABY\",\n",
    "    \"CDL_HIKKAKEMOD\",\n",
    "    \"CDL_RISEFALL3METHODS\",\n",
    "    \"CDL_KICKING\",\n",
    "    \"CDL_DOJI_10_0.1\",\n",
    "    \"CDL_HARAMI\",\n",
    "    \"CDL_3BLACKCROWS\",\n",
    "    \"CDL_LADDERBOTTOM\",\n",
    "    \"CDL_INNECK\",\n",
    "    \"CDL_SHORTLINE\",\n",
    "    \"CDL_3OUTSIDE\",\n",
    "    \"CDL_MORNINGSTAR\",\n",
    "    \"CDL_HIGHWAVE\",\n",
    "    \"CDL_LONGLINE\",\n",
    "    \"CDL_TRISTAR\",\n",
    "    \"CDL_UNIQUE3RIVER\",\n",
    "    \"CDL_2CROWS\",\n",
    "    \"CDL_THRUSTING\",\n",
    "    \"CDL_COUNTERATTACK\",\n",
    "    \"CDL_MORNINGDOJISTAR\",\n",
    "    \"CDL_INVERTEDHAMMER\",\n",
    "    \"CDL_CLOSINGMARUBOZU\",\n",
    "    \"CDL_HANGINGMAN\",\n",
    "    \"CDL_TASUKIGAP\",\n",
    "    \"CDL_3STARSINSOUTH\",\n",
    "    \"CDL_ENGULFING\",\n",
    "    \"CDL_DRAGONFLYDOJI\",\n",
    "    \"CDL_HAMMER\",\n",
    "    \"CDL_GRAVESTONEDOJI\",\n",
    "    \"CDL_MATHOLD\",\n",
    "    \"CDL_TAKURI\",\n",
    "    \"CDL_IDENTICAL3CROWS\",\n",
    "    \"CDL_EVENINGSTAR\",\n",
    "    \"CDL_STALLEDPATTERN\",\n",
    "    \"CDL_SEPARATINGLINES\",\n",
    "]\n",
    "\n",
    "ZERO_ONE_COL = [\n",
    "    \"DEC_1\",\n",
    "    \"THERMOl_20_2_0.5\",\n",
    "    \"TTM_TRND_6\",\n",
    "    \"INC_1\",\n",
    "    \"THERMOs_20_2_0.5\",\n",
    "    \"AMATe_LR_8_21_2\",\n",
    "    \"SQZPRO_OFF\",\n",
    "    \"AOBV_LR_2\",\n",
    "    \"SQZPRO_ON_NARROW\",\n",
    "    \"PSARr_0.02_0.2\",\n",
    "    \"SQZPRO_ON_NORMAL\",\n",
    "    \"AMATe_SR_8_21_2\",\n",
    "    \"AOBV_SR_2\",\n",
    "    \"SQZ_OFF\",\n",
    "    \"SUPERTd_7_3.0\",\n",
    "    \"SQZ_ON\",\n",
    "    \"SQZPRO_ON_WIDE\",\n",
    "]\n",
    "\n",
    "PRICE_COL = [\n",
    "    \"DCL_20_20\",\n",
    "    \"ISB_26\",\n",
    "    \"ABER_ZG_5_15\",\n",
    "    \"high\",\n",
    "    \"HA_open\",\n",
    "    \"weighted\",\n",
    "    \"ABER_SG_5_15\",\n",
    "    \"WMA_10\",\n",
    "    \"EMA_10\",\n",
    "    \"ICS_26\",\n",
    "    \"HWU\",\n",
    "    \"low\",\n",
    "    \"SUPERT_7_3.0\",\n",
    "    \"MIDPOINT_2\",\n",
    "    \"VWAP_D\",\n",
    "    \"RMA_10\",\n",
    "    \"HLC3\",\n",
    "    \"TRIMA_10\",\n",
    "    \"HWL\",\n",
    "    \"HA_low\",\n",
    "    \"HA_high\",\n",
    "    \"IKS_26\",\n",
    "    \"KCBe_20_2\",\n",
    "    \"ACCBM_20\",\n",
    "    \"MEDIAN_30\",\n",
    "    \"CKSPl_10_3_20\",\n",
    "    \"KCLe_20_2\",\n",
    "    \"ACCBL_20\",\n",
    "    \"LR_14\",\n",
    "    \"KAMA_10_2_30\",\n",
    "    \"HL2\",\n",
    "    \"TEMA_10\",\n",
    "    \"QTL_30_0.5\",\n",
    "    \"ZL_EMA_10\",\n",
    "    \"VWMA_10\",\n",
    "    \"ITS_9\",\n",
    "    \"LDECAY_5\",\n",
    "    \"DCM_20_20\",\n",
    "    \"JMA_7_0\",\n",
    "    \"VIDYA_14\",\n",
    "    \"DEMA_10\",\n",
    "    \"ACCBU_20\",\n",
    "    \"ISA_9\",\n",
    "    \"FWMA_10\",\n",
    "    \"CKSPs_10_3_20\",\n",
    "    \"HILO_13_21\",\n",
    "    \"WCP\",\n",
    "    \"HMA_10\",\n",
    "    \"PWMA_10\",\n",
    "    \"HA_close\",\n",
    "    \"BBM_5_2.0\",\n",
    "    \"SWMA_10\",\n",
    "    \"MCGD_10\",\n",
    "    \"DCU_20_20\",\n",
    "    \"MIDPRICE_2\",\n",
    "    \"T3_10_0.7\",\n",
    "    \"HWM\",\n",
    "    \"SSF_10_2\",\n",
    "    \"OHLC4\",\n",
    "    \"HWMA_0.2_0.1_0.1\",\n",
    "    \"open\",\n",
    "    \"BBL_5_2.0\",\n",
    "    \"ALMA_10_6.0_0.85\",\n",
    "    \"SINWMA_14\",\n",
    "    \"close\",\n",
    "    \"ABER_XG_5_15\",\n",
    "    \"BBU_5_2.0\",\n",
    "    \"KCUe_20_2\",\n",
    "    \"SMA_10\",\n",
    "]\n",
    "\n",
    "OTHER_COL = [\n",
    "    \"STCmacd_10_12_26_0.5\",\n",
    "    \"WILLR_14\",\n",
    "    \"ENTP_10\",\n",
    "    \"INERTIA_20_14\",\n",
    "    \"J_9_3\",\n",
    "    \"TRIX_30_9\",\n",
    "    \"AROONOSC_14\",\n",
    "    \"AO_5_34\",\n",
    "    \"MACDs_12_26_9\",\n",
    "    \"QQE_14_5_4.236\",\n",
    "    \"trades\",\n",
    "    \"PVOs_12_26_9\",\n",
    "    \"SMIs_5_20_5\",\n",
    "    \"QQE_14_5_4.236_RSIMA\",\n",
    "    \"KVOs_34_55_13\",\n",
    "    \"STOCHRSIk_14_14_3_3\",\n",
    "    \"MFI_14\",\n",
    "    \"MACD_12_26_9\",\n",
    "    \"EOM_14_100000000\",\n",
    "    \"PVT\",\n",
    "    \"STOCHk_14_3_3\",\n",
    "    \"SMIo_5_20_5\",\n",
    "    \"DPO_20\",\n",
    "    \"high_Z_30_1\",\n",
    "    \"PVOh_12_26_9\",\n",
    "    \"AR_26\",\n",
    "    \"PSARaf_0.02_0.2\",\n",
    "    \"BBP_5_2.0\",\n",
    "    \"FISHERT_9_1\",\n",
    "    \"CG_10\",\n",
    "    \"NATR_14\",\n",
    "    \"STDEV_30\",\n",
    "    \"CMF_20\",\n",
    "    \"THERMOma_20_2_0.5\",\n",
    "    \"PPO_12_26_9\",\n",
    "    \"LOGRET_1\",\n",
    "    \"PPOs_12_26_9\",\n",
    "    \"BBB_5_2.0\",\n",
    "    \"D_9_3\",\n",
    "    \"OBVe_4\",\n",
    "    \"BOP\",\n",
    "    \"OBV_max_2\",\n",
    "    \"volume_asset_buyer_maker\",\n",
    "    \"FISHERTs_9_1\",\n",
    "    \"SQZ_20_2.0_20_1.5\",\n",
    "    \"ABER_ATR_5_15\",\n",
    "    \"VTXM_14\",\n",
    "    \"OBVe_12\",\n",
    "    \"K_9_3\",\n",
    "    \"TSIs_13_25_13\",\n",
    "    \"volume\",\n",
    "    \"MASSI_9_25\",\n",
    "    \"RVGIs_14_4\",\n",
    "    \"ZS_30\",\n",
    "    \"PCTRET_1\",\n",
    "    \"RVGI_14_4\",\n",
    "    \"THERMO_20_2_0.5\",\n",
    "    \"close_Z_30_1\",\n",
    "    \"open_Z_30_1\",\n",
    "    \"BEARP_13\",\n",
    "    \"KURT_30\",\n",
    "    \"DMP_14\",\n",
    "    \"STOCHRSId_14_14_3_3\",\n",
    "    \"MACDh_12_26_9\",\n",
    "    \"RVI_14\",\n",
    "    \"VAR_30\",\n",
    "    \"SLOPE_1\",\n",
    "    \"BR_26\",\n",
    "    \"CCI_14_0.015\",\n",
    "    \"TRUERANGE_1\",\n",
    "    \"SMI_5_20_5\",\n",
    "    \"COPC_11_14_10\",\n",
    "    \"BULLP_13\",\n",
    "    \"RSX_14\",\n",
    "    \"VTXP_14\",\n",
    "    \"volume_asset_buyer_taker_ratio\",\n",
    "    \"AROOND_14\",\n",
    "    \"AD\",\n",
    "    \"PGO_14\",\n",
    "    \"EBSW_40_10\",\n",
    "    \"MAD_30\",\n",
    "    \"STCstoch_10_12_26_0.5\",\n",
    "    \"KST_10_15_20_30_10_10_10_15\",\n",
    "    \"STOCHd_14_3_3\",\n",
    "    \"KVO_34_55_13\",\n",
    "    \"PDIST\",\n",
    "    \"ROC_10\",\n",
    "    \"STC_10_12_26_0.5\",\n",
    "    \"CMO_14\",\n",
    "    \"PVI_1\",\n",
    "    \"PSL_12\",\n",
    "    \"volume_asset_buyer_taker\",\n",
    "    \"DMN_14\",\n",
    "    \"UO_7_14_28\",\n",
    "    \"volume_asset\",\n",
    "    \"TRIXs_30_9\",\n",
    "    \"PVR\",\n",
    "    \"PPOh_12_26_9\",\n",
    "    \"KSTs_9\",\n",
    "    \"SKEW_30\",\n",
    "    \"MOM_10\",\n",
    "    \"ADOSC_3_10\",\n",
    "    \"BIAS_SMA_26\",\n",
    "    \"UI_14\",\n",
    "    \"low_Z_30_1\",\n",
    "    \"NVI_1\",\n",
    "    \"EFI_13\",\n",
    "    \"PVOL\",\n",
    "    \"OBV_min_2\",\n",
    "    \"ATRr_14\",\n",
    "    \"VHF_28\",\n",
    "    \"SQZPRO_20_2.0_20_2_1.5_1\",\n",
    "    \"APO_12_26\",\n",
    "    \"CTI_12\",\n",
    "    \"CFO_9\",\n",
    "    \"PVO_12_26_9\",\n",
    "    \"trades_full\",\n",
    "    \"RSI_14\",\n",
    "    \"CHOP_14_1_100\",\n",
    "    \"ADX_14\",\n",
    "    \"AROONU_14\",\n",
    "    \"TSI_13_25_13\",\n",
    "    \"QS_10\",\n",
    "    \"OBV\",\n",
    "    \"ER_10\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a753a487",
   "metadata": {},
   "source": [
    "Functions from earlier notebooks and an improved data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0247931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom encoder for writing numpy arrays to a json file\n",
    "class JSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (datetime, date)):\n",
    "            return obj.isoformat()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "# wraper on pandas concatenation catching error of concatenation of empty list and list containing none values\n",
    "def safe_concat(dataframes):\n",
    "    dataframes = [df for df in dataframes if df is not None]\n",
    "    try:\n",
    "        return pd.concat(dataframes)\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6486a5",
   "metadata": {},
   "source": [
    "#### By putting together the analyzes presented in previous notebooks, we can wrap the entire process of building a dataset into one class. Also, with very minor modifications, we can use multithreading for the most time-consuming stages - data collection and generation of technical analysis indicators and, consequently, speed up the process of generating dateset several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "811d0274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OHLCDataSet:\n",
    "\n",
    "    \"\"\"Class that enables the generation of datasets,\n",
    "    with we can wrap the entire process of building a dataset of Binance trading data with technical analysis.\n",
    "\n",
    "    Example:\n",
    "    ```ods = OHLCDataSet(\n",
    "    pairs=[('eth', 'usdt'), ('btc', 'usdt')],\n",
    "    start_date=date(2022, 3, 10),\n",
    "    end_date=date(2022, 3, 13),\n",
    "    train_end=date(2022, 3, 12),\n",
    "    ohlc_intervals = [2, 4, 10],\n",
    "    multithreading=True,\n",
    "    )\n",
    "    ```\n",
    "    Args:\n",
    "        pairs: List of trading pairs symbol.\n",
    "        start_date: Date From when to collect data.\n",
    "        end_date: Date by which data is collected.\n",
    "        train_end. The date when we consider that the training set ends, important because some global statistics,\n",
    "        such as the average, are calculated only on the basis of training data.\n",
    "        base_time_offset: Time unit which constitutes the database of the data set.\n",
    "        All the time intervals used must be an integer multiple of this value.\n",
    "        ohlc_intervals: Defines with which time step size the time series are created,\n",
    "        multiples of the base unit base_time_offset.\n",
    "        drop_n_first_rows: How many leading rows are thrown from the DataFrame,\n",
    "        the values may be different for different intervals\n",
    "        dataset_seq_len: Defines the number of time steps that the final samples used to train the models will have.\n",
    "        The length of the time series can be different for different intervals.\n",
    "        drop_columns_names: The names of the columns that we do not want to use may be different for different intervals\n",
    "        columns: a dictionary containing the following keys:\n",
    "        monotonic_col, price_col, pair_col, zero_one_col, candle_col, other_col\n",
    "        which contain assignments of particular column names to different classes of features.\n",
    "        If one is not provided, or the entire dictionary is not provided,\n",
    "        the assignment will be generated automatically.\n",
    "        If you do not know your data well, it is better to leave it blank.\n",
    "        multithreading: Use of multithreading for data retrieval and preprocessing\n",
    "        transform_mode: Specifies the procedure of normalization of features not correlated with the price (often fat-tail distributions)\n",
    "        The choices are ‘power’, ‘quintile’ or ‘mix’ (default).\n",
    "        ‘power’ uses PowerTransformer, ‘quintile’ uses QuantileTransformer and ‘mix’ uses PowerTransformer\n",
    "        unless there is an error then uses QuantileTransformer.\n",
    "        binance_base_url: url for binance data, by default 'https://data.binance.vision'.\n",
    "        data_dir: Where to save downloaded files, by default 'data'\n",
    "        fill_gaps_limit: removes columns that have gaps longer than fill_gaps_limit, by default 5\n",
    "        inverse_error_quintiles: the quintile values ​​for which the inverse transformation error is to be calculated,\n",
    "        by default [q for q in np.arange(0,1,0.1)] + [0.95, 0.99, 0.999]\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pairs,\n",
    "        start_date,\n",
    "        end_date,\n",
    "        train_end,\n",
    "        base_time_offset=pd.tseries.offsets.Minute(),\n",
    "        price_scaling_method=\"standardised_max_pct_change\",\n",
    "        ohlc_intervals=None,\n",
    "        drop_n_first_rows=None,\n",
    "        dataset_seq_len=None,\n",
    "        drop_columns_names=None,\n",
    "        columns=None,\n",
    "        multithreading=False,\n",
    "        keep_unchanged_df=False,\n",
    "        transform_mode=\"mix\",\n",
    "        binance_base_url=\"https://data.binance.vision\",\n",
    "        data_dir=\"data\",\n",
    "        fill_gaps_limit=5,\n",
    "        inverse_error_quintiles=None,\n",
    "    ):\n",
    "        self.inverse_error_quintiles = inverse_error_quintiles\n",
    "        if self.inverse_error_quintiles is None:\n",
    "            self.inverse_error_quintiles = [q for q in np.arange(0, 1, 0.1)] + [\n",
    "                0.95,\n",
    "                0.99,\n",
    "                0.999,\n",
    "            ]\n",
    "        self.binance_base_url = binance_base_url\n",
    "        self.data_dir = data_dir\n",
    "        self.fill_gaps_limit = fill_gaps_limit\n",
    "        if ohlc_intervals is None:\n",
    "            ohlc_intervals = [1, 10, 60]\n",
    "        for interval in ohlc_intervals[1:]:\n",
    "            if interval % ohlc_intervals[0]:\n",
    "                raise ValueError(\n",
    "                    \"successive intervals must be integers of multiples of the basic interval\"\n",
    "                )\n",
    "        if drop_n_first_rows is None:\n",
    "            drop_n_first_rows = {interval: 96 for interval in ohlc_intervals}\n",
    "        if dataset_seq_len is None:\n",
    "            dataset_seq_len = {interval: 30 for interval in ohlc_intervals}\n",
    "        if drop_columns_names is None:\n",
    "            drop_columns_names = {\n",
    "                interval: [\n",
    "                    \"DPO_20\",\n",
    "                    \"ICS_26\",\n",
    "                    \"EOM_14_100000000\",\n",
    "                    \"QQEl_14_5_4.236\",\n",
    "                    \"QQEs_14_5_4.236\",\n",
    "                    \"SQZPRO_NO\",\n",
    "                    \"SQZ_NO\",\n",
    "                ]\n",
    "                for interval in ohlc_intervals\n",
    "            }\n",
    "        if columns is None:\n",
    "            columns = {}\n",
    "\n",
    "        self.pairs = [asset + base for asset, base in pairs]\n",
    "        self.asset_base_pairs = pairs\n",
    "\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.train_end = train_end\n",
    "        self.base_time_offset = base_time_offset\n",
    "        self.price_scaling_method = price_scaling_method\n",
    "        self.ohlc_intervals = ohlc_intervals\n",
    "        self.drop_n_first_rows = drop_n_first_rows\n",
    "        self.dataset_seq_len = dataset_seq_len\n",
    "        self.drop_columns_names = drop_columns_names\n",
    "\n",
    "        # You can provide dict that describes different type of columns, or this can be calculated from data\n",
    "        self.monotonic_col = columns.get(\"monotonic_col\")\n",
    "        self.price_col = columns.get(\"price_col\")\n",
    "        # this one set of columns is an exception, it must be specified before preprocessing\n",
    "        # because we use it at an early stage of data cleansing which is needed to calculated the rest of the column types\n",
    "        self.pair_col = columns.get(\n",
    "            \"pair_col\",\n",
    "            [\n",
    "                \"HILOl_13_21\",\n",
    "                \"HILOs_13_21\",\n",
    "                \"PSARl_0.02_0.2\",\n",
    "                \"PSARs_0.02_0.2\",\n",
    "                \"QQEl_14_5_4.236\",\n",
    "                \"QQEs_14_5_4.236\",\n",
    "                \"SUPERTl_7_3.0\",\n",
    "                \"SUPERTs_7_3.0\",\n",
    "            ],\n",
    "        )\n",
    "        self.time_col = [\n",
    "            \"HOUR_SIN\",\n",
    "            \"HOUR_COS\",\n",
    "            \"DAY_SIN\",\n",
    "            \"DAY_COS\",\n",
    "            \"WEEK_SIN\",\n",
    "            \"WEEK_COS\",\n",
    "            \"YEAR_SIN\",\n",
    "            \"YEAR_COS\",\n",
    "        ]\n",
    "        self.zero_one_col = columns.get(\"zero_one_col\")\n",
    "        self.candle_col = columns.get(\"candle_col\")\n",
    "        self.other_col = columns.get(\"other_col\")\n",
    "\n",
    "        # dataset parameters, set in self.make_dataset\n",
    "        self.all_columns = None\n",
    "        self.power_factors = None\n",
    "        self.price_std = None\n",
    "\n",
    "        self.ohlc_data = {\n",
    "            pair: {\n",
    "                interval: {\n",
    "                    self.ohlc_intervals[0] * step: {}\n",
    "                    for step in range(interval // self.ohlc_intervals[0])\n",
    "                }\n",
    "                for interval in self.ohlc_intervals\n",
    "            }\n",
    "            for pair in self.pairs\n",
    "        }\n",
    "\n",
    "        self.multithreading = multithreading\n",
    "        self.keep_unchanged_df = keep_unchanged_df\n",
    "        self.unchanged_df = None\n",
    "        self.transform_mode = transform_mode\n",
    "        # Dictionary to store the time taken by individual tasks\n",
    "        self.timing = {}\n",
    "\n",
    "    def print_lenght(self):\n",
    "        i = 0\n",
    "        for interval in self.ohlc_intervals:\n",
    "            for step in range(interval // self.ohlc_intervals[0]):\n",
    "                for pair in self.pairs:\n",
    "                    print(\n",
    "                        i,\n",
    "                        pair,\n",
    "                        interval,\n",
    "                        step,\n",
    "                        self.ohlc_data[pair][interval][self.ohlc_intervals[0] * step]\n",
    "                        .to_numpy(dtype=np.float64)\n",
    "                        .shape,\n",
    "                        self.ohlc_data[pair][interval][\n",
    "                            self.ohlc_intervals[0] * step\n",
    "                        ].index.nunique(),\n",
    "                    )\n",
    "                    i += 1\n",
    "\n",
    "    def make_dataset(self):\n",
    "        # loading and basic cleaning data\n",
    "        start = monotonic()\n",
    "        start_make_dataset = start\n",
    "        self._load_data()\n",
    "        self.timing[\"load_time\"] = monotonic() - start\n",
    "\n",
    "        start = monotonic()\n",
    "        self._clear_data()\n",
    "        self.timing[\"clear_time\"] = monotonic() - start\n",
    "        start = monotonic()\n",
    "        self._remove_no_variance_features()\n",
    "        self.timing[\"remove_no_variance_time\"] = monotonic() - start\n",
    "        start = monotonic()\n",
    "        self._remove_redundant_features()\n",
    "        self.timing[\"remove_redundant_features_time\"] = monotonic() - start\n",
    "\n",
    "        start = monotonic()\n",
    "        # organizing features by various types (various preprocessing for various types)\n",
    "        self.all_columns = self.get_all_columns()\n",
    "        if self.monotonic_col is None:\n",
    "            self.monotonic_col = self.get_monotonic_col()\n",
    "        if self.zero_one_col is None:\n",
    "            self.zero_one_col = self.get_zero_one_col()\n",
    "        if self.candle_col is None:\n",
    "            self.candle_col = self.get_candle_col()\n",
    "        self.pair_col = self.get_pairs_col()\n",
    "        start_inner = monotonic()\n",
    "        if self.price_col is None:\n",
    "            self.price_col = self.get_price_like_col()\n",
    "        self.timing[\"get_price_columns_time\"] = monotonic() - start_inner\n",
    "        self.timing[\"get_columns_time\"] = monotonic() - start\n",
    "\n",
    "        start = monotonic()\n",
    "        self.monotonic_col = [\n",
    "            col for col in self.monotonic_col if col not in self.candle_col\n",
    "        ]\n",
    "        self.pair_col = [col for col in self.pair_col if col not in self.candle_col]\n",
    "        self.zero_one_col = [\n",
    "            col for col in self.zero_one_col if col not in self.candle_col\n",
    "        ]\n",
    "        self.price_col = [\n",
    "            col\n",
    "            for col in self.price_col\n",
    "            if all(\n",
    "                [\n",
    "                    col not in cols\n",
    "                    for cols in [\n",
    "                        self.monotonic_col,\n",
    "                        self.pair_col,\n",
    "                        self.candle_col,\n",
    "                        self.zero_one_col,\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "        self.other_col = [\n",
    "            col\n",
    "            for col in self.all_columns\n",
    "            if all(\n",
    "                [\n",
    "                    col not in cols\n",
    "                    for cols in [\n",
    "                        self.monotonic_col,\n",
    "                        self.pair_col,\n",
    "                        self.candle_col,\n",
    "                        self.zero_one_col,\n",
    "                        self.price_col,\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "        self.timing[\"columns_organizing_time\"] = monotonic() - start\n",
    "        self._drop_first_unused()\n",
    "        self._trim_longer_dataframes(front_trim=False)\n",
    "\n",
    "        self.timing[\"drop_unused_time\"] = monotonic() - start\n",
    "        if self.keep_unchanged_df:\n",
    "            self.unchanged_df = copy.deepcopy(self.ohlc_data)\n",
    "\n",
    "        start = monotonic()\n",
    "        self._drop_monotonic_col()\n",
    "        self.timing[\"drop_monotonic_time\"] = monotonic() - start\n",
    "\n",
    "        start = monotonic()\n",
    "        self._normalize_candle_col()\n",
    "        self.timing[\"normalize_candle_time\"] = monotonic() - start\n",
    "\n",
    "        start = monotonic()\n",
    "        self.power_factors = self._normalize_other_col(mode=self.transform_mode)\n",
    "        self.timing[\"normalize_other_time\"] = monotonic() - start\n",
    "\n",
    "        start = monotonic()\n",
    "        self.price_std = self.get_price_std()\n",
    "        self.timing[\"get_price_std_time\"] = monotonic() - start\n",
    "\n",
    "        start = monotonic()\n",
    "        self._add_time_features()\n",
    "        self.all_columns.extend(self.time_col)\n",
    "        self.timing[\"add_time_features_time\"] = monotonic() - start\n",
    "        self.timing[\"whole_process_time\"] = monotonic() - start_make_dataset\n",
    "\n",
    "    def _process_single_data(self, pair, single_date, verbose=False):\n",
    "        if verbose:\n",
    "            print(pair, single_date.strftime(\"%Y-%m-%d\"))\n",
    "        try:\n",
    "            day_trades = get_trades(\n",
    "                pair,\n",
    "                single_date.year,\n",
    "                str(single_date.month).zfill(2),\n",
    "                str(single_date.day).zfill(2),\n",
    "                binance_base_url=self.binance_base_url,\n",
    "                trades_dir=join(self.data_dir, \"trades\"),\n",
    "            )\n",
    "        except zipfile.BadZipfile:\n",
    "            print(\"BadZipfile, probably wrong date\")\n",
    "            return None\n",
    "\n",
    "        for interval in self.ohlc_intervals:\n",
    "            for step in range(interval // self.ohlc_intervals[0]):\n",
    "                offset = self.ohlc_intervals[0] * step\n",
    "                day_ohlc = trades2ohlc(\n",
    "                    day_trades,\n",
    "                    resampling_frequency=interval,\n",
    "                    offset=offset,\n",
    "                    base_time_offset=self.base_time_offset,\n",
    "                )\n",
    "                self.ohlc_data[pair][interval][offset][single_date] = day_ohlc\n",
    "\n",
    "    def _process_ohlc_ta(self, pair, interval, offset, save_to_csv=False):\n",
    "        self.ohlc_data[pair][interval][offset] = merge_ohlc(\n",
    "            self.ohlc_data[pair][interval][offset]\n",
    "        )\n",
    "        self.ohlc_data[pair][interval][offset].ta.strategy(ta.AllStrategy)\n",
    "\n",
    "        # remove don`t used collumns\n",
    "        for col in self.drop_columns_names[interval]:\n",
    "            self.ohlc_data[pair][interval][offset].pop(col)\n",
    "\n",
    "        # Some technical indicators needs some previous data points to calculate value,\n",
    "        # by dropping first n rows we put more features in our dataset.\n",
    "        self.ohlc_data[pair][interval][offset] = self.ohlc_data[pair][interval][\n",
    "            offset\n",
    "        ].iloc[self.drop_n_first_rows[interval] :]\n",
    "        # remove columns that are still empty at the beginning after the set number of leading\n",
    "        # rows have been dropped. Also removes columns that have gaps longer than self.fill_gaps_limit (5)\n",
    "        # These infinities are a numerical error, when calculating technical indicators,\n",
    "        # we want to treat them as not a number\n",
    "        self.ohlc_data[pair][interval][offset].replace(\n",
    "            [np.inf, -np.inf], np.nan, inplace=True\n",
    "        )\n",
    "        for col in self.ohlc_data[pair][interval][offset]:\n",
    "            if col in self.pair_col:\n",
    "                continue\n",
    "            self.ohlc_data[pair][interval][offset].fillna(\n",
    "                method=\"ffill\", limit=self.fill_gaps_limit, inplace=True\n",
    "            )\n",
    "            if self.ohlc_data[pair][interval][offset][col].isnull().values.any():\n",
    "                self.ohlc_data[pair][interval][offset].pop(col)\n",
    "\n",
    "    def _load_data(self):\n",
    "        pd_list = [\n",
    "            (pair, single_date)\n",
    "            for pair in self.pairs\n",
    "            for single_date in daterange(self.start_date, self.end_date)\n",
    "        ]\n",
    "        params_list = [\n",
    "            (pair, interval, self.ohlc_intervals[0] * step)\n",
    "            for pair in self.pairs\n",
    "            for interval in self.ohlc_intervals\n",
    "            for step in range(interval // self.ohlc_intervals[0])\n",
    "        ]\n",
    "\n",
    "        if self.multithreading:\n",
    "            with ThreadPoolExecutor(20) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(self._process_single_data, *pair_data)\n",
    "                    for pair_data in pd_list\n",
    "                ]\n",
    "                wait(futures, return_when=ALL_COMPLETED)\n",
    "                futures = [\n",
    "                    executor.submit(self._process_ohlc_ta, *params)\n",
    "                    for params in params_list\n",
    "                ]\n",
    "                wait(futures, return_when=ALL_COMPLETED)\n",
    "        else:\n",
    "            for pair_data in pd_list:\n",
    "                self._process_single_data(*pair_data)\n",
    "            for params in params_list:\n",
    "                self._process_ohlc_ta(*params)\n",
    "\n",
    "    def _drop_monotonic_col(self):\n",
    "        for pair in self.pairs:\n",
    "            for interval in self.ohlc_intervals:\n",
    "                for step in range(interval // self.ohlc_intervals[0]):\n",
    "                    offset = self.ohlc_intervals[0] * step\n",
    "                    for col in self.ohlc_data[pair][interval][offset]:\n",
    "                        if col in self.monotonic_col:\n",
    "                            self.ohlc_data[pair][interval][offset].pop(col)\n",
    "\n",
    "    # For the same pair and the same dataframe interval, they should be the same length for all offsets.\n",
    "    # Mainly due to later vectorization and performance issues\n",
    "    def _trim_longer_dataframes(self, front_trim=True):\n",
    "        for pair in self.pairs:\n",
    "            for interval in self.ohlc_intervals[1:]:\n",
    "                min_sequence_len = min(\n",
    "                    [\n",
    "                        len(\n",
    "                            self.ohlc_data[pair][interval][\n",
    "                                self.ohlc_intervals[0] * step\n",
    "                            ]\n",
    "                        )\n",
    "                        for step in range(interval // self.ohlc_intervals[0])\n",
    "                    ]\n",
    "                )\n",
    "                for step in range(interval // self.ohlc_intervals[0]):\n",
    "                    offset = self.ohlc_intervals[0] * step\n",
    "                    if len(self.ohlc_data[pair][interval][offset]) > min_sequence_len:\n",
    "                        to_drop = (\n",
    "                            len(self.ohlc_data[pair][interval][offset])\n",
    "                            - min_sequence_len\n",
    "                        )\n",
    "                        if to_drop > 0:\n",
    "                            if front_trim:\n",
    "                                to_drop = self.ohlc_data[pair][interval][offset].index[\n",
    "                                    :to_drop\n",
    "                                ]\n",
    "                                self.ohlc_data[pair][interval][offset].drop(\n",
    "                                    index=to_drop, inplace=True\n",
    "                                )\n",
    "                            else:\n",
    "                                to_drop = self.ohlc_data[pair][interval][offset].index[\n",
    "                                    -to_drop:\n",
    "                                ]\n",
    "                                self.ohlc_data[pair][interval][offset].drop(\n",
    "                                    index=to_drop, inplace=True\n",
    "                                )\n",
    "\n",
    "    def _clear_data(self):\n",
    "        # If some pair (we allow different sets of features for different intervals) dont have some column,\n",
    "        # this column also must be dropped from others dataframes\n",
    "        for interval in self.ohlc_intervals:\n",
    "            columns_lists = []\n",
    "            for pair in self.pairs:\n",
    "                for step in range(interval // self.ohlc_intervals[0]):\n",
    "                    offset = self.ohlc_intervals[0] * step\n",
    "                    columns_lists.append(\n",
    "                        self.ohlc_data[pair][interval][offset].columns.values\n",
    "                    )\n",
    "            all_columns = []\n",
    "            [all_columns.extend(cols) for cols in columns_lists]\n",
    "            all_columns = list(set(all_columns))\n",
    "            common_columns = [\n",
    "                c for c in all_columns if all([c in cc for cc in columns_lists])\n",
    "            ]\n",
    "\n",
    "            for pair in self.pairs:\n",
    "                for step in range(interval // self.ohlc_intervals[0]):\n",
    "                    offset = self.ohlc_intervals[0] * step\n",
    "                    for col in self.ohlc_data[pair][interval][offset]:\n",
    "                        if col not in common_columns:\n",
    "                            self.ohlc_data[pair][interval][offset].pop(col)\n",
    "\n",
    "        self._trim_longer_dataframes()\n",
    "        # Some time steps will never be used, so they can be deleted\n",
    "        for pair in self.pairs:\n",
    "            last_base_timestamp = self.ohlc_data[pair][self.ohlc_intervals[0]][0].index[\n",
    "                -1\n",
    "            ]\n",
    "            first_base_timestamp = self.ohlc_data[pair][self.ohlc_intervals[0]][\n",
    "                0\n",
    "            ].index[0]\n",
    "            first_usable_timestamp = first_base_timestamp\n",
    "\n",
    "            for interval in self.ohlc_intervals[1:]:\n",
    "                for step in range(interval // self.ohlc_intervals[0]):\n",
    "                    offset = self.ohlc_intervals[0] * step\n",
    "                    self.ohlc_data[pair][interval][offset] = self.ohlc_data[pair][\n",
    "                        interval\n",
    "                    ][offset][\n",
    "                        (\n",
    "                            self.ohlc_data[pair][interval][offset].index\n",
    "                            + (interval - self.ohlc_intervals[0])\n",
    "                            * self.base_time_offset\n",
    "                            <= last_base_timestamp\n",
    "                        )\n",
    "                        & (\n",
    "                            self.ohlc_data[pair][interval][offset].index\n",
    "                            >= first_base_timestamp\n",
    "                        )\n",
    "                    ]\n",
    "                first_usable_timestamp = max(\n",
    "                    self.ohlc_data[pair][interval][self.ohlc_intervals[0]].index[0]\n",
    "                    + self.dataset_seq_len[interval] * interval * self.base_time_offset,\n",
    "                    first_usable_timestamp,\n",
    "                )\n",
    "\n",
    "            self.ohlc_data[pair][self.ohlc_intervals[0]][0] = self.ohlc_data[pair][\n",
    "                self.ohlc_intervals[0]\n",
    "            ][0][\n",
    "                self.ohlc_data[pair][self.ohlc_intervals[0]][0].index\n",
    "                >= first_usable_timestamp\n",
    "            ]\n",
    "            first_base_timestamp = self.ohlc_data[pair][self.ohlc_intervals[0]][\n",
    "                0\n",
    "            ].index[0]\n",
    "            for interval in self.ohlc_intervals[1:]:\n",
    "                for step in range(interval // self.ohlc_intervals[0]):\n",
    "                    offset = self.ohlc_intervals[0] * step\n",
    "                    self.ohlc_data[pair][interval][offset] = self.ohlc_data[pair][\n",
    "                        interval\n",
    "                    ][offset][\n",
    "                        self.ohlc_data[pair][interval][offset].index\n",
    "                        + self.dataset_seq_len[interval]\n",
    "                        * interval\n",
    "                        * self.base_time_offset\n",
    "                        >= first_base_timestamp\n",
    "                    ]\n",
    "\n",
    "    def _remove_no_variance_features(self):\n",
    "        for interval in self.ohlc_intervals:\n",
    "\n",
    "            columns_lists = []\n",
    "            for pair in self.pairs:\n",
    "                for step in range(interval // self.ohlc_intervals[0]):\n",
    "                    offset = self.ohlc_intervals[0] * step\n",
    "                    columns_lists.append(\n",
    "                        self.ohlc_data[pair][interval][offset].columns.values\n",
    "                    )\n",
    "            all_columns = []\n",
    "            [all_columns.extend(cols) for cols in columns_lists]\n",
    "            all_columns = list(set(all_columns))\n",
    "\n",
    "            no_variance_columns = []\n",
    "            for col in all_columns:\n",
    "                col_values = []\n",
    "                for pair in self.pairs:\n",
    "                    for step in range(interval // self.ohlc_intervals[0]):\n",
    "                        offset = self.ohlc_intervals[0] * step\n",
    "                        col_values.extend(\n",
    "                            self.ohlc_data[pair][interval][offset][col].unique()\n",
    "                        )\n",
    "                col_values = list(set(col_values))\n",
    "                if len(col_values) <= 1:\n",
    "                    no_variance_columns.append(col)\n",
    "\n",
    "            for pair in self.pairs:\n",
    "                for step in range(interval // self.ohlc_intervals[0]):\n",
    "                    offset = self.ohlc_intervals[0] * step\n",
    "                    for col in no_variance_columns:\n",
    "                        self.ohlc_data[pair][interval][offset].pop(col)\n",
    "\n",
    "        return self.ohlc_data\n",
    "\n",
    "    def _remove_redundant_features(self):\n",
    "        for interval in self.ohlc_intervals:\n",
    "            cor = (\n",
    "                pd.concat(\n",
    "                    [\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                self.ohlc_data[pair][interval][\n",
    "                                    self.ohlc_intervals[0] * step\n",
    "                                ]\n",
    "                                for pair in self.pairs\n",
    "                            ]\n",
    "                        )\n",
    "                        for step in range(interval // self.ohlc_intervals[0])\n",
    "                    ]\n",
    "                )\n",
    "                .corr()\n",
    "                .abs()\n",
    "            )\n",
    "\n",
    "            upper_tri = cor.where(np.triu(np.ones(cor.shape), k=1).astype(bool))\n",
    "            to_drop = [\n",
    "                column for column in upper_tri.columns if any(upper_tri[column] == 1)\n",
    "            ]\n",
    "\n",
    "            for pair in self.pairs:\n",
    "                for step in range(interval // self.ohlc_intervals[0]):\n",
    "                    offset = self.ohlc_intervals[0] * step\n",
    "                    self.ohlc_data[pair][interval][offset].drop(\n",
    "                        to_drop, axis=1, inplace=True\n",
    "                    )\n",
    "\n",
    "    def get_all_columns(self):\n",
    "        all_columns = []\n",
    "        [\n",
    "            all_columns.extend(\n",
    "                self.ohlc_data[self.pairs[0]][interval][0].columns.values\n",
    "            )\n",
    "            for interval in self.ohlc_intervals\n",
    "        ]\n",
    "        return sorted(list(set(all_columns)))\n",
    "\n",
    "    def get_monotonic_col(self):\n",
    "\n",
    "        monotonic_features = {}\n",
    "        for interval in self.ohlc_intervals:\n",
    "            for feature in self.all_columns:\n",
    "                if feature in self.ohlc_data[self.pairs[0]][interval][0]:\n",
    "                    is_monotonic = True\n",
    "                    for pair in self.pairs:\n",
    "                        for step in range(interval // self.ohlc_intervals[0]):\n",
    "                            offset = self.ohlc_intervals[0] * step\n",
    "                            is_monotonic = (\n",
    "                                self.ohlc_data[pair][interval][offset][\n",
    "                                    feature\n",
    "                                ].is_monotonic\n",
    "                                or self.ohlc_data[pair][interval][offset][\n",
    "                                    feature\n",
    "                                ].is_monotonic_decreasing\n",
    "                            )\n",
    "                            if not is_monotonic:\n",
    "                                break\n",
    "                        if not is_monotonic:\n",
    "                            break\n",
    "                    if is_monotonic:\n",
    "                        monotonic_features.setdefault(feature, []).append(True)\n",
    "                    else:\n",
    "                        monotonic_features.setdefault(feature, []).append(False)\n",
    "\n",
    "        monotonic_features = [\n",
    "            feature\n",
    "            for feature, is_monotonic in monotonic_features.items()\n",
    "            if all(is_monotonic)\n",
    "        ]\n",
    "        return monotonic_features\n",
    "\n",
    "    def get_price_like_col(self, price_col=\"close\", price_treshold=0.55):\n",
    "        price_series = pd.concat(\n",
    "            [\n",
    "                pd.concat(\n",
    "                    [\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                self.ohlc_data[pair][interval][offset][price_col]\n",
    "                                for offset in [\n",
    "                                    self.ohlc_intervals[0] * step\n",
    "                                    for step in range(\n",
    "                                        interval // self.ohlc_intervals[0]\n",
    "                                    )\n",
    "                                ]\n",
    "                                if price_col in self.ohlc_data[pair][interval][offset]\n",
    "                            ]\n",
    "                        )\n",
    "                        for pair in self.pairs\n",
    "                    ]\n",
    "                )\n",
    "                for interval in self.ohlc_intervals\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        col_stats = {}\n",
    "        for col in self.all_columns:\n",
    "            if col.startswith(\"CDL_\"):\n",
    "                continue\n",
    "            col_series = pd.concat(\n",
    "                [\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            pd.concat(\n",
    "                                [\n",
    "                                    self.ohlc_data[pair][interval][offset][col]\n",
    "                                    if col in self.ohlc_data[pair][interval][offset]\n",
    "                                    else pd.Series(\n",
    "                                        [\n",
    "                                            None\n",
    "                                            for _ in range(\n",
    "                                                len(\n",
    "                                                    self.ohlc_data[pair][interval][\n",
    "                                                        offset\n",
    "                                                    ]\n",
    "                                                )\n",
    "                                            )\n",
    "                                        ],\n",
    "                                        name=col,\n",
    "                                        dtype=float,\n",
    "                                        index=self.ohlc_data[pair][interval][\n",
    "                                            offset\n",
    "                                        ].index,\n",
    "                                    )\n",
    "                                    for offset in [\n",
    "                                        self.ohlc_intervals[0] * step\n",
    "                                        for step in range(\n",
    "                                            interval // self.ohlc_intervals[0]\n",
    "                                        )\n",
    "                                    ]\n",
    "                                ]\n",
    "                            )\n",
    "                            for pair in self.pairs\n",
    "                        ]\n",
    "                    )\n",
    "                    for interval in self.ohlc_intervals\n",
    "                ]\n",
    "            )\n",
    "            col_stats[col] = {\n",
    "                \"mean\": (col_series / price_series).mean(),\n",
    "                \"std\": (col_series / price_series).std(),\n",
    "            }\n",
    "\n",
    "        price_features = [\n",
    "            col\n",
    "            for col in col_stats.keys()\n",
    "            if abs(col_stats[col][\"mean\"] - 1) <= price_treshold\n",
    "            and col_stats[col][\"std\"] <= price_treshold\n",
    "        ]\n",
    "        return price_features\n",
    "\n",
    "    def get_candle_col(self):\n",
    "        candle_col = []\n",
    "        for col in self.all_columns:\n",
    "            if col.startswith(\"CDL_\"):\n",
    "                candle_col.append(col)\n",
    "        return candle_col\n",
    "\n",
    "    def get_zero_one_col(self):\n",
    "        zero_one_features = []\n",
    "        for col in self.all_columns:\n",
    "            if col.startswith(\"CDL_\"):\n",
    "                continue\n",
    "            col_series = safe_concat(\n",
    "                [\n",
    "                    safe_concat(\n",
    "                        [\n",
    "                            safe_concat(\n",
    "                                [\n",
    "                                    self.ohlc_data[pair][interval][offset][col]\n",
    "                                    for offset in [\n",
    "                                        self.ohlc_intervals[0] * step\n",
    "                                        for step in range(\n",
    "                                            interval // self.ohlc_intervals[0]\n",
    "                                        )\n",
    "                                    ]\n",
    "                                    if col in self.ohlc_data[pair][interval][offset]\n",
    "                                ]\n",
    "                            )\n",
    "                            for pair in self.pairs\n",
    "                        ]\n",
    "                    )\n",
    "                    for interval in self.ohlc_intervals\n",
    "                ]\n",
    "            )\n",
    "            if col_series.isin([-1, 0, 1]).all():\n",
    "                zero_one_features.append(col)\n",
    "        return zero_one_features\n",
    "\n",
    "    def get_pairs_col(self):\n",
    "        pairs = [\n",
    "            \"HILOl_13_21\",\n",
    "            \"HILOs_13_21\",\n",
    "            \"PSARl_0.02_0.2\",\n",
    "            \"PSARs_0.02_0.2\",\n",
    "            \"QQEl_14_5_4.236\",\n",
    "            \"QQEs_14_5_4.236\",\n",
    "            \"SUPERTl_7_3.0\",\n",
    "            \"SUPERTs_7_3.0\",\n",
    "        ]\n",
    "        return [col for col in self.all_columns if col in pairs]\n",
    "\n",
    "    def _normalize_candle_col(self):\n",
    "        for interval in self.ohlc_intervals:\n",
    "            for pair in self.pairs:\n",
    "                for step in range(interval // self.ohlc_intervals[0]):\n",
    "                    offset = self.ohlc_intervals[0] * step\n",
    "                    candle_col_exist = [\n",
    "                        col\n",
    "                        for col in self.candle_col\n",
    "                        if col in self.ohlc_data[pair][interval][offset]\n",
    "                    ]\n",
    "                    self.ohlc_data[pair][interval][offset][candle_col_exist] /= 100\n",
    "\n",
    "    def _normalize_other_col(self, mode=\"mix\"):\n",
    "        if mode == \"power\":\n",
    "            return self._power_transform_other_col()\n",
    "        elif mode == \"quintile\":\n",
    "            return self._quintile_other_col()\n",
    "        elif mode == \"mix\":\n",
    "            return self._power_quintile_transform_other_col()\n",
    "        else:\n",
    "            raise ValueError(f\"_normalize_other_col, mode {mode} not supported\")\n",
    "\n",
    "    def _quintile_other_col(self):\n",
    "        result = {}\n",
    "        for interval in self.ohlc_intervals:\n",
    "            other_columns = [\n",
    "                c\n",
    "                for c in self.ohlc_data[self.pairs[0]][interval][0].columns.values\n",
    "                if c in self.other_col\n",
    "            ]\n",
    "            sc_quint = QuantileTransformer(\n",
    "                n_quantiles=1000,\n",
    "                output_distribution=\"normal\",\n",
    "                ignore_implicit_zeros=False,\n",
    "                subsample=100000,\n",
    "            )\n",
    "            sc_quint.fit(\n",
    "                pd.concat(\n",
    "                    [\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                self.ohlc_data[pair][interval][\n",
    "                                    self.ohlc_intervals[0] * step\n",
    "                                ].loc[self.start_date : self.train_end][other_columns]\n",
    "                                for pair in self.pairs\n",
    "                            ]\n",
    "                        )\n",
    "                        for step in range(interval // self.ohlc_intervals[0])\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            errors = []\n",
    "            for pair in self.pairs:\n",
    "                for step in range(interval // self.ohlc_intervals[0]):\n",
    "                    offset = self.ohlc_intervals[0] * step\n",
    "                    transformed = sc_quint.transform(\n",
    "                        self.ohlc_data[pair][interval][offset][other_columns]\n",
    "                    )\n",
    "                    restored = sc_quint.inverse_transform(transformed)\n",
    "                    original = self.ohlc_data[pair][interval][offset][\n",
    "                        other_columns\n",
    "                    ].to_numpy()\n",
    "                    inverse_error = restored - original\n",
    "                    inverse_error = np.absolute(\n",
    "                        np.divide(\n",
    "                            inverse_error,\n",
    "                            original,\n",
    "                            out=np.zeros_like(inverse_error),\n",
    "                            where=original != 0,\n",
    "                        )\n",
    "                    )\n",
    "                    errors.append(inverse_error)\n",
    "                    self.ohlc_data[pair][interval][offset][other_columns] = transformed\n",
    "\n",
    "            errors = np.concatenate(errors, axis=0)\n",
    "            max_errors = errors.max(axis=0)\n",
    "            errors = np.array(\n",
    "                [\n",
    "                    [\n",
    "                        np.quantile(errors, q_val, axis=0)\n",
    "                        for q_val in self.inverse_error_quintiles\n",
    "                    ]\n",
    "                ]\n",
    "            ).T\n",
    "            errors = errors.reshape((errors.shape[0], -1))\n",
    "\n",
    "            result[interval] = {\n",
    "                name: {\n",
    "                    \"quantiles\": quantiles_,\n",
    "                    \"mode\": \"quantiles\",\n",
    "                    \"inverse_error_quantiles\": q_error,\n",
    "                    \"max_inverse_error\": m_error,\n",
    "                }\n",
    "                for name, quantiles_, q_error, m_error in zip(\n",
    "                    other_columns, sc_quint.quantiles_.T, errors, max_errors\n",
    "                )\n",
    "            }\n",
    "            result[interval][\"used_quintiles\"] = self.inverse_error_quintiles\n",
    "        return result\n",
    "\n",
    "    def _power_quintile_transform_other_col(self):\n",
    "        result = {}\n",
    "        for interval in self.ohlc_intervals:\n",
    "            result[interval] = {}\n",
    "\n",
    "            other_columns = [\n",
    "                c\n",
    "                for c in self.ohlc_data[self.pairs[0]][interval][0].columns.values\n",
    "                if c in self.other_col\n",
    "            ]\n",
    "\n",
    "            sc_quint = QuantileTransformer(\n",
    "                n_quantiles=1000,\n",
    "                output_distribution=\"normal\",\n",
    "                ignore_implicit_zeros=False,\n",
    "                subsample=100000,\n",
    "            )\n",
    "            sc_power = PowerTransformer(method=\"yeo-johnson\")\n",
    "\n",
    "            for i, column_name in enumerate(other_columns):\n",
    "                merged_data = pd.concat(\n",
    "                    [\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                self.ohlc_data[pair][interval][\n",
    "                                    self.ohlc_intervals[0] * step\n",
    "                                ].loc[self.start_date : self.train_end][[column_name]]\n",
    "                                for pair in self.pairs\n",
    "                            ]\n",
    "                        )\n",
    "                        for step in range(interval // self.ohlc_intervals[0])\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                sc_quint.fit(merged_data)\n",
    "                sc_power.fit(merged_data)\n",
    "\n",
    "                errors = []\n",
    "                if sc_power._scaler.var_[0] > 0:\n",
    "                    for pair in self.pairs:\n",
    "                        for step in range(interval // self.ohlc_intervals[0]):\n",
    "                            offset = self.ohlc_intervals[0] * step\n",
    "                            transformed = sc_power.transform(\n",
    "                                self.ohlc_data[pair][interval][offset][[column_name]]\n",
    "                            )\n",
    "                            restored = sc_power.inverse_transform(transformed)\n",
    "                            original = self.ohlc_data[pair][interval][offset][\n",
    "                                [column_name]\n",
    "                            ].to_numpy()\n",
    "                            inverse_error = restored - original\n",
    "                            inverse_error = np.absolute(\n",
    "                                np.divide(\n",
    "                                    inverse_error,\n",
    "                                    original,\n",
    "                                    out=np.zeros_like(inverse_error),\n",
    "                                    where=original != 0,\n",
    "                                )\n",
    "                            )\n",
    "                            errors.append(inverse_error)\n",
    "                            self.ohlc_data[pair][interval][offset][\n",
    "                                [column_name]\n",
    "                            ] = transformed\n",
    "\n",
    "                    eshapes = [e.shape for e in errors]\n",
    "                    errors = np.concatenate(errors, axis=0)\n",
    "                    result[interval][column_name] = {\n",
    "                        \"mean\": sc_power._scaler.mean_,\n",
    "                        \"var\": sc_power._scaler.var_,\n",
    "                        \"lambda\": sc_power.lambdas_,\n",
    "                        \"mode\": \"power\",\n",
    "                        \"inverse_error_quantiles\": {\n",
    "                            q_val: np.quantile(errors, q_val)\n",
    "                            for q_val in self.inverse_error_quintiles\n",
    "                        },\n",
    "                        \"max_inverse_error\": errors.max(),\n",
    "                    }\n",
    "\n",
    "                else:\n",
    "                    for pair in self.pairs:\n",
    "                        for step in range(interval // self.ohlc_intervals[0]):\n",
    "                            offset = self.ohlc_intervals[0] * step\n",
    "                            transformed = sc_quint.transform(\n",
    "                                self.ohlc_data[pair][interval][offset][[column_name]]\n",
    "                            )\n",
    "                            restored = sc_quint.inverse_transform(transformed)\n",
    "                            original = self.ohlc_data[pair][interval][offset][\n",
    "                                [column_name]\n",
    "                            ].to_numpy()\n",
    "                            inverse_error = np.absolute(restored - original)\n",
    "                            inverse_error = np.divide(\n",
    "                                inverse_error,\n",
    "                                original,\n",
    "                                out=np.zeros_like(inverse_error),\n",
    "                                where=original != 0,\n",
    "                            )\n",
    "                            errors.append(inverse_error)\n",
    "                            self.ohlc_data[pair][interval][offset][\n",
    "                                [column_name]\n",
    "                            ] = transformed\n",
    "\n",
    "                    eshapes = [e.shape for e in errors]\n",
    "                    errors = np.concatenate(errors, axis=0)\n",
    "                    result[interval][column_name] = {\n",
    "                        \"quantiles\": sc_quint.quantiles_,\n",
    "                        \"mode\": \"quantiles\",\n",
    "                        \"inverse_error_quantiles\": {\n",
    "                            q_val: np.quantile(errors, q_val)\n",
    "                            for q_val in self.inverse_error_quintiles\n",
    "                        },\n",
    "                        \"max_inverse_error\": errors.max(),\n",
    "                    }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _power_transform_other_col(self):\n",
    "        result = {}\n",
    "        for interval in self.ohlc_intervals:\n",
    "            other_columns = [\n",
    "                c\n",
    "                for c in self.ohlc_data[self.pairs[0]][interval][0].columns.values\n",
    "                if c in self.other_col\n",
    "            ]\n",
    "            sc_power = PowerTransformer()\n",
    "            sc_power.fit(\n",
    "                pd.concat(\n",
    "                    [\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                self.ohlc_data[pair][interval][\n",
    "                                    self.ohlc_intervals[0] * step\n",
    "                                ].loc[self.start_date : self.train_end][other_columns]\n",
    "                                for pair in self.pairs\n",
    "                            ]\n",
    "                        )\n",
    "                        for step in range(interval // self.ohlc_intervals[0])\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            errors = []\n",
    "            for pair in self.pairs:\n",
    "                for step in range(interval // self.ohlc_intervals[0]):\n",
    "                    offset = self.ohlc_intervals[0] * step\n",
    "                    transformed = sc_power.transform(\n",
    "                        self.ohlc_data[pair][interval][offset][other_columns]\n",
    "                    )\n",
    "                    restored = sc_power.inverse_transform(transformed)\n",
    "                    original = self.ohlc_data[pair][interval][offset][\n",
    "                        other_columns\n",
    "                    ].to_numpy()\n",
    "                    inverse_error = restored - original\n",
    "                    inverse_error = np.absolute(\n",
    "                        np.divide(\n",
    "                            inverse_error,\n",
    "                            original,\n",
    "                            out=np.zeros_like(inverse_error),\n",
    "                            where=original != 0,\n",
    "                        )\n",
    "                    )\n",
    "                    errors.append(inverse_error)\n",
    "                    self.ohlc_data[pair][interval][offset][other_columns] = transformed\n",
    "\n",
    "            errors = np.concatenate(errors, axis=0)\n",
    "            max_errors = errors.max(axis=0)\n",
    "            errors = np.array(\n",
    "                [\n",
    "                    [\n",
    "                        np.quantile(errors, q_val, axis=0)\n",
    "                        for q_val in self.inverse_error_quintiles\n",
    "                    ]\n",
    "                ]\n",
    "            ).T\n",
    "            errors = errors.reshape((errors.shape[0], -1))\n",
    "\n",
    "            result[interval] = {\n",
    "                name: {\n",
    "                    \"mean\": mean_,\n",
    "                    \"var\": var_,\n",
    "                    \"lambda\": lambda_,\n",
    "                    \"mode\": \"power\",\n",
    "                    \"inverse_error_quantiles\": q_error,\n",
    "                    \"max_inverse_error\": m_error,\n",
    "                }\n",
    "                for name, mean_, var_, lambda_, q_error, m_error in zip(\n",
    "                    other_columns,\n",
    "                    sc_power._scaler.mean_,\n",
    "                    sc_power._scaler.var_,\n",
    "                    sc_power.lambdas_,\n",
    "                    errors,\n",
    "                    max_errors,\n",
    "                )\n",
    "            }\n",
    "            result[interval][\"used_quintiles\"] = self.inverse_error_quintiles\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_price_std(self):\n",
    "\n",
    "        pct_change_result = {}\n",
    "        max_pct_change_result = {}\n",
    "        last_pct_change_result = {}\n",
    "        price_cols = [\"open\", \"low\", \"high\", \"close\", \"weighted\"]\n",
    "        for price_col in price_cols:\n",
    "            pct_change_result[price_col] = {\n",
    "                interval: pd.concat(\n",
    "                    [\n",
    "                        pd.DataFrame().assign(\n",
    "                            **{\n",
    "                                str(i): self.ohlc_data[pair][interval][offset]\n",
    "                                .loc[self.start_date : self.train_end][price_col]\n",
    "                                .pct_change(periods=-i)\n",
    "                                for i in range(1, self.dataset_seq_len[interval])\n",
    "                            }\n",
    "                        )\n",
    "                        for pair in self.pairs\n",
    "                        for offset in [\n",
    "                            self.ohlc_intervals[0] * step\n",
    "                            for step in range(interval // self.ohlc_intervals[0])\n",
    "                        ]\n",
    "                    ]\n",
    "                )\n",
    "                .stack()\n",
    "                .std()\n",
    "                for interval in self.ohlc_intervals\n",
    "            }\n",
    "\n",
    "            max_pct_change_result[price_col] = {\n",
    "                interval: pd.concat(\n",
    "                    [\n",
    "                        pd.DataFrame().assign(\n",
    "                            **{\n",
    "                                str(i): self.ohlc_data[pair][interval][offset]\n",
    "                                .loc[self.start_date : self.train_end][price_col]\n",
    "                                .pct_change(periods=-i)\n",
    "                                for i in range(1, self.dataset_seq_len[interval])\n",
    "                            }\n",
    "                        )\n",
    "                        for pair in self.pairs\n",
    "                        for offset in [\n",
    "                            self.ohlc_intervals[0] * step\n",
    "                            for step in range(interval // self.ohlc_intervals[0])\n",
    "                        ]\n",
    "                    ]\n",
    "                )\n",
    "                .abs()\n",
    "                .max(axis=1)\n",
    "                .std()\n",
    "                for interval in self.ohlc_intervals\n",
    "            }\n",
    "\n",
    "            last_pct_change_result[price_col] = {\n",
    "                interval: pd.concat(\n",
    "                    [\n",
    "                        self.ohlc_data[pair][interval][offset]\n",
    "                        .loc[self.start_date : self.train_end][price_col]\n",
    "                        .pct_change(periods=self.dataset_seq_len[interval] - 1)\n",
    "                        for pair in self.pairs\n",
    "                        for offset in [\n",
    "                            self.ohlc_intervals[0] * step\n",
    "                            for step in range(interval // self.ohlc_intervals[0])\n",
    "                        ]\n",
    "                    ]\n",
    "                ).std()\n",
    "                for interval in self.ohlc_intervals\n",
    "            }\n",
    "\n",
    "        result = {\n",
    "            interval: {\n",
    "                price_col: {\n",
    "                    \"pct_change\": pct_change_result[price_col][interval],\n",
    "                    \"max_pct_change\": max_pct_change_result[price_col][interval],\n",
    "                    \"last_pct_change\": last_pct_change_result[price_col][interval],\n",
    "                }\n",
    "                for price_col in price_cols\n",
    "            }\n",
    "            for interval in self.ohlc_intervals\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _drop_first_unused(self):\n",
    "        for interval in self.ohlc_intervals[1:]:\n",
    "            freq = interval // self.ohlc_intervals[0]\n",
    "            offsets = [\n",
    "                self.ohlc_intervals[0] * step\n",
    "                for step in range(interval // self.ohlc_intervals[0])\n",
    "            ]\n",
    "            offsets = sorted(offsets, reverse=True)[:-1]\n",
    "            offsets.insert(0, 0)\n",
    "            for pair in self.pairs:\n",
    "                base_list_len = len(self.ohlc_data[pair][self.ohlc_intervals[0]][0])\n",
    "                first_n_pos = list(\n",
    "                    range(\n",
    "                        base_list_len\n",
    "                        - self.dataset_seq_len[self.ohlc_intervals[0]]\n",
    "                        + 1,\n",
    "                        base_list_len\n",
    "                        - self.dataset_seq_len[self.ohlc_intervals[0]]\n",
    "                        + 1\n",
    "                        - freq,\n",
    "                        -1,\n",
    "                    )\n",
    "                )\n",
    "                for pos in first_n_pos:\n",
    "                    offset = offsets[(pos - 1) % freq]\n",
    "                    idx = -((pos + freq - 1) // freq)\n",
    "                    to_drop = (\n",
    "                        len(self.ohlc_data[pair][interval][offset])\n",
    "                        + idx\n",
    "                        - self.dataset_seq_len[interval]\n",
    "                        + 1\n",
    "                    )\n",
    "                    self.ohlc_data[pair][interval][offset].drop(\n",
    "                        index=self.ohlc_data[pair][interval][offset].index[:to_drop],\n",
    "                        inplace=True,\n",
    "                    )\n",
    "\n",
    "    # The time in seconds is not a useful feature. I have assumed a few fairly standard periodicity that may be present\n",
    "    def _add_time_features(self, add_timestamp=False):\n",
    "        hour = 60 * 60\n",
    "        day = 24 * hour\n",
    "        week = 7 * day\n",
    "        year = (365.2425) * day\n",
    "\n",
    "        for interval in self.ohlc_intervals:\n",
    "            for pair in self.pairs:\n",
    "                for step in range(interval // self.ohlc_intervals[0]):\n",
    "                    offset = self.ohlc_intervals[0] * step\n",
    "\n",
    "                    # now we dont assume intervals shorther than 1s sow we divide by 10**9 to convert nanoseconds to seconds\n",
    "                    # it should be changed if we want to use shorter interwals.\n",
    "                    # we add half of base_time_offset to get time of\n",
    "                    timestamp_middle = (\n",
    "                        self.ohlc_data[pair][interval][offset].index.asi8\n",
    "                        + offset * self.base_time_offset.nanos / 2\n",
    "                    )\n",
    "                    seconds = timestamp_middle / 10**9\n",
    "\n",
    "                    self.ohlc_data[pair][interval][offset][\"HOUR_SIN\"] = np.sin(\n",
    "                        seconds * (2 * np.pi / hour)\n",
    "                    )\n",
    "                    self.ohlc_data[pair][interval][offset][\"HOUR_COS\"] = np.cos(\n",
    "                        seconds * (2 * np.pi / hour)\n",
    "                    )\n",
    "                    self.ohlc_data[pair][interval][offset][\"DAY_SIN\"] = np.sin(\n",
    "                        seconds * (2 * np.pi / day)\n",
    "                    )\n",
    "                    self.ohlc_data[pair][interval][offset][\"DAY_COS\"] = np.cos(\n",
    "                        seconds * (2 * np.pi / day)\n",
    "                    )\n",
    "                    self.ohlc_data[pair][interval][offset][\"WEEK_SIN\"] = np.sin(\n",
    "                        seconds * (2 * np.pi / week)\n",
    "                    )\n",
    "                    self.ohlc_data[pair][interval][offset][\"WEEK_COS\"] = np.cos(\n",
    "                        seconds * (2 * np.pi / week)\n",
    "                    )\n",
    "                    self.ohlc_data[pair][interval][offset][\"YEAR_SIN\"] = np.sin(\n",
    "                        seconds * (2 * np.pi / year)\n",
    "                    )\n",
    "                    self.ohlc_data[pair][interval][offset][\"YEAR_COS\"] = np.cos(\n",
    "                        seconds * (2 * np.pi / year)\n",
    "                    )\n",
    "\n",
    "                    if add_timestamp:\n",
    "                        self.ohlc_data[pair][interval][offset][\n",
    "                            \"TIMESTAMP\"\n",
    "                        ] = timestamp_middle\n",
    "\n",
    "    def get_ohlc_data(self):\n",
    "        return self.ohlc_data\n",
    "\n",
    "    def get_arrays(self):\n",
    "\n",
    "        base = np.concatenate(\n",
    "            [\n",
    "                self.ohlc_data[pair][self.ohlc_intervals[0]][0].to_numpy(\n",
    "                    dtype=np.float64\n",
    "                )\n",
    "                for pair in self.pairs\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        other_intervals = [\n",
    "            np.stack(\n",
    "                [\n",
    "                    np.concatenate(\n",
    "                        [\n",
    "                            self.ohlc_data[pair][interval][\n",
    "                                self.ohlc_intervals[0] * step\n",
    "                            ].to_numpy(dtype=np.float64)\n",
    "                            for pair in self.pairs\n",
    "                        ]\n",
    "                    )\n",
    "                    for step in range(interval // self.ohlc_intervals[0])\n",
    "                ]\n",
    "            )\n",
    "            for interval in self.ohlc_intervals[1:]\n",
    "        ]\n",
    "\n",
    "        return base, *other_intervals\n",
    "\n",
    "    def get_metadata(self):\n",
    "\n",
    "        metadata = {\n",
    "            \"timing\": self.timing,\n",
    "            \"pairs\": self.pairs,\n",
    "            \"asset_base_pairs\": self.asset_base_pairs,\n",
    "            \"start_date\": self.start_date,\n",
    "            \"end_date\": self.end_date,\n",
    "            \"train_end\": self.train_end,\n",
    "            \"base_time_offset\": self.base_time_offset.__class__.__name__,\n",
    "            \"ohlc_intervals\": self.ohlc_intervals,\n",
    "            \"drop_n_first_rows\": self.drop_n_first_rows,\n",
    "            \"dataset_seq_len\": self.dataset_seq_len,\n",
    "            \"drop_columns_names\": self.drop_columns_names,\n",
    "            \"all_columns\": self.all_columns,\n",
    "            \"monotonic_col\": self.monotonic_col,\n",
    "            \"price_col\": self.price_col,\n",
    "            \"pair_col\": self.pair_col,\n",
    "            \"zero_one_col\": self.zero_one_col,\n",
    "            \"candle_col\": self.candle_col,\n",
    "            \"other_col\": self.other_col,\n",
    "            \"dataframe_stats\": {\n",
    "                interval: {\n",
    "                    \"columns_order\": self.ohlc_data[self.pairs[0]][interval][\n",
    "                        0\n",
    "                    ].columns.values,\n",
    "                    \"offsets\": [\n",
    "                        self.ohlc_intervals[0] * step\n",
    "                        for step in range(interval // self.ohlc_intervals[0])\n",
    "                    ],\n",
    "                    \"power_factors\": self.power_factors[interval],\n",
    "                    \"price_std\": self.price_std[interval],\n",
    "                    \"pairs\": {\n",
    "                        pair: {\n",
    "                            \"first_timestamps\": [\n",
    "                                self.ohlc_data[pair][interval][\n",
    "                                    self.ohlc_intervals[0] * step\n",
    "                                ].index[0]\n",
    "                                for step in range(interval // self.ohlc_intervals[0])\n",
    "                            ],\n",
    "                            \"last_timestamps\": [\n",
    "                                self.ohlc_data[pair][interval][\n",
    "                                    self.ohlc_intervals[0] * step\n",
    "                                ].index[-1]\n",
    "                                for step in range(interval // self.ohlc_intervals[0])\n",
    "                            ],\n",
    "                            \"last_train_timestamps\": [\n",
    "                                self.ohlc_data[pair][interval][\n",
    "                                    self.ohlc_intervals[0] * step\n",
    "                                ]\n",
    "                                .loc[: self.train_end]\n",
    "                                .index[-1]\n",
    "                                for step in range(interval // self.ohlc_intervals[0])\n",
    "                            ],\n",
    "                            \"lenghts\": [\n",
    "                                len(\n",
    "                                    self.ohlc_data[pair][interval][\n",
    "                                        self.ohlc_intervals[0] * step\n",
    "                                    ]\n",
    "                                )\n",
    "                                for step in range(interval // self.ohlc_intervals[0])\n",
    "                            ],\n",
    "                            \"train_steps\": [\n",
    "                                len(\n",
    "                                    self.ohlc_data[pair][interval][\n",
    "                                        self.ohlc_intervals[0] * step\n",
    "                                    ].loc[: self.train_end]\n",
    "                                )\n",
    "                                for step in range(interval // self.ohlc_intervals[0])\n",
    "                            ],\n",
    "                        }\n",
    "                        for pair in self.pairs\n",
    "                    },\n",
    "                }\n",
    "                for interval in self.ohlc_intervals\n",
    "            },\n",
    "        }\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def save_arrays(self, filename):\n",
    "        if len(filename.split(\".\")) == 1:\n",
    "            filename += \".npz\"\n",
    "        if len(filename.split(\".\")) == 2 and filename.split(\".\")[-1] != \"npz\":\n",
    "            raise ValueError(\"file name must end with .npz\")\n",
    "        np.savez(\n",
    "            filename,\n",
    "            **{str(k): v for k, v in zip(self.ohlc_intervals, self.get_arrays())},\n",
    "        )\n",
    "\n",
    "    def save_compressed_arrays(self, filename):\n",
    "        if len(filename.split(\".\")) == 1:\n",
    "            filename += \".npz\"\n",
    "        if len(filename.split(\".\")) == 2 and filename.split(\".\")[-1] != \"npz\":\n",
    "            raise ValueError(\"file name must end with .npz\")\n",
    "        np.savez_compressed(\n",
    "            filename,\n",
    "            **{str(k): v for k, v in zip(self.ohlc_intervals, self.get_arrays())},\n",
    "        )\n",
    "\n",
    "    def save(self, filename):\n",
    "        if len(filename.split(\".\")) == 1:\n",
    "            filename += \".pkl\"\n",
    "        if len(filename.split(\".\")) == 2 and filename.split(\".\")[-1] != \"pkl\":\n",
    "            raise ValueError(\"file name must end with .pkl\")\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(self, file)\n",
    "\n",
    "    def save_metadata(self, filename, to_pickle=True):\n",
    "        if to_pickle:\n",
    "            if len(filename.split(\".\")) == 1:\n",
    "                filename += \".pkl\"\n",
    "            if len(filename.split(\".\")) == 2 and filename.split(\".\")[-1] != \"pkl\":\n",
    "                raise ValueError(\"file name must end with .pkl\")\n",
    "            with open(filename, \"wb\") as file:\n",
    "                pickle.dump(self.get_metadata(), file)\n",
    "        else:\n",
    "            if len(filename.split(\".\")) == 1:\n",
    "                filename += \".json\"\n",
    "            if len(filename.split(\".\")) == 2 and filename.split(\".\")[-1] != \"json\":\n",
    "                raise ValueError(\"file name must end with .json\")\n",
    "            with open(filename, \"w\") as fp:\n",
    "                json.dump(self.get_metadata(), fp, cls=JSONEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff0b9fc-77c9-41fe-9884-ddc36973be91",
   "metadata": {},
   "source": [
    "Now, we instantiate OHLCDataSet with custom config.\n",
    "I changed pairs providing format: [('eth', 'usdt'), ('btc', 'usdt')] contains more info than ['ethusdt', 'btcusdt'],\n",
    "it is easier to extract asset and base name - usefull in further encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80bdf372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify the exact names of the columns we want to use,\n",
    "# then we will force the selection of columns and their categorization into appropriate groups.\n",
    "# The dictionary can be given partially or not at all, then the features and their assignment\n",
    "# to groups will be determined automatically, based on the data, this approach is the most robustnes\n",
    "# columns={\n",
    "#     'monotonic_col': MONOTONIC_COL,\n",
    "#     'price_col': PRICE_COL,\n",
    "#     'pair_col': PAIR_COL,\n",
    "#     'zero_one_col': ZERO_ONE_COL,\n",
    "#     'candle_col': CANDLE_COL,\n",
    "#     'other_col': OTHER_COL,\n",
    "# }\n",
    "columns = None\n",
    "\n",
    "ods = OHLCDataSet(\n",
    "    pairs=[(\"eth\", \"usdt\"), (\"btc\", \"usdt\"), (\"doge\", \"usdt\")],\n",
    "    start_date=date(2022, 4, 1),\n",
    "    end_date=date(2022, 4, 30),\n",
    "    train_end=date(2022, 4, 25),\n",
    "    ohlc_intervals=[10, 20, 60],\n",
    "    multithreading=True,\n",
    "    keep_unchanged_df=True,\n",
    "    transform_mode=\"mix\",\n",
    "    base_time_offset=pd.tseries.offsets.Minute(),\n",
    "    columns=columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5ce16-b070-41a5-aa8c-f50e0775ff91",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download data and prepare dataset\n",
    "res = ods.make_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "683dca21-6624-477b-b26a-151ea58bdc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'add_time_features_time': 0.037396013001853134,\n",
      " 'clear_time': 0.10598202200344531,\n",
      " 'columns_organizing_time': 0.0003433620004216209,\n",
      " 'drop_monotonic_time': 0.07275125600062893,\n",
      " 'drop_unused_time': 0.034627368000656134,\n",
      " 'get_columns_time': 1.1785183839965612,\n",
      " 'get_price_columns_time': 0.47894473200358334,\n",
      " 'get_price_std_time': 3.578840794001735,\n",
      " 'load_time': 69.07267884699831,\n",
      " 'normalize_candle_time': 0.16356760100097745,\n",
      " 'normalize_other_time': 11.04634865999833,\n",
      " 'remove_no_variance_time': 1.1368558080030198,\n",
      " 'remove_redundant_features_time': 2.9867632950008556,\n",
      " 'whole_process_time': 89.44456547699883}\n"
     ]
    }
   ],
   "source": [
    "pprint(ods.timing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f55116",
   "metadata": {},
   "source": [
    "For features characterized by a log normal or other fat-tail distribution, we use poweer transform (Yeo-Johnson transformation) or the quintile transformation. During these transformations, the error of restoring the original values is checked. Checking this error helps to make sure that the transformation was successful. We can see that the error for 0.9 quintiles are very small, so a significant number of features are mapped with very high accuracy, however, there are outliers, most often caused by the quintile transformation which is strictly based on data and badly reproduces data beyond the data fit range. We have such a situation when we have values ​​in the test set that exceed the range of the training set. Statistically, the larger the training set we have, the lower the probability of large errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88e3d90a-78e6-4c60-a0ad-a9a5a37e42be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 99% quintile of inverse error for the analyzed features: \n",
      "{10: 0.00018689653831357233, 20: 0.0001222590640398664, 60: 0.00032492385173927017}\n",
      "highest 99% quintile of inverse error for the analyzed features: \n",
      "{10: 0.02017959183673483, 20: 0.014285714285714235, 60: 0.03846153846153853}\n",
      "lowest 99% quintile of inverse error for the analyzed features: \n",
      "{10: 0.0, 20: -0.0, 60: 0.0}\n"
     ]
    }
   ],
   "source": [
    "# The average, highest and lowest 99% quintile of inverse error for the analyzed features\n",
    "quintile = 0.99\n",
    "\n",
    "average = {\n",
    "    interval: sum(\n",
    "        [\n",
    "            d[\"inverse_error_quantiles\"][quintile]\n",
    "            for d in ods.get_metadata()[\"dataframe_stats\"][interval][\n",
    "                \"power_factors\"\n",
    "            ].values()\n",
    "        ]\n",
    "    )\n",
    "    / len(\n",
    "        [\n",
    "            d[\"inverse_error_quantiles\"][quintile]\n",
    "            for d in ods.get_metadata()[\"dataframe_stats\"][interval][\n",
    "                \"power_factors\"\n",
    "            ].values()\n",
    "        ]\n",
    "    )\n",
    "    for interval in ods.ohlc_intervals\n",
    "}\n",
    "\n",
    "highest = {\n",
    "    interval: max(\n",
    "        [\n",
    "            d[\"inverse_error_quantiles\"][quintile]\n",
    "            for d in ods.get_metadata()[\"dataframe_stats\"][interval][\n",
    "                \"power_factors\"\n",
    "            ].values()\n",
    "        ]\n",
    "    )\n",
    "    for interval in ods.ohlc_intervals\n",
    "}\n",
    "\n",
    "lowest = {\n",
    "    interval: min(\n",
    "        [\n",
    "            d[\"inverse_error_quantiles\"][quintile]\n",
    "            for d in ods.get_metadata()[\"dataframe_stats\"][interval][\n",
    "                \"power_factors\"\n",
    "            ].values()\n",
    "        ]\n",
    "    )\n",
    "    for interval in ods.ohlc_intervals\n",
    "}\n",
    "\n",
    "print(f\"average 99% quintile of inverse error for the analyzed features: \\n{average}\")\n",
    "print(f\"highest 99% quintile of inverse error for the analyzed features: \\n{highest}\")\n",
    "print(f\"lowest 99% quintile of inverse error for the analyzed features: \\n{lowest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c52598-3cf7-4018-aec1-0852c380c463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 99% quintile of inverse error for the analyzed features: \n",
      "{10: 0.00018689653831357233, 20: 0.0001222590640398664, 60: 0.00032492385173927017}\n",
      "highest 99% quintile of inverse error for the analyzed features: \n",
      "{10: 0.02017959183673483, 20: 0.014285714285714235, 60: 0.03846153846153853}\n",
      "lowest 99% quintile of inverse error for the analyzed features: \n",
      "{10: 0.0, 20: -0.0, 60: 0.0}\n"
     ]
    }
   ],
   "source": [
    "# The average, highest and lowest 99% quintile of inverse error for the analyzed features\n",
    "quintile = 0.99\n",
    "\n",
    "average = {\n",
    "    interval: sum(\n",
    "        [\n",
    "            d[\"inverse_error_quantiles\"][quintile]\n",
    "            for d in ods.get_metadata()[\"dataframe_stats\"][interval][\n",
    "                \"power_factors\"\n",
    "            ].values()\n",
    "        ]\n",
    "    )\n",
    "    / len(\n",
    "        [\n",
    "            d[\"inverse_error_quantiles\"][quintile]\n",
    "            for d in ods.get_metadata()[\"dataframe_stats\"][interval][\n",
    "                \"power_factors\"\n",
    "            ].values()\n",
    "        ]\n",
    "    )\n",
    "    for interval in ods.ohlc_intervals\n",
    "}\n",
    "\n",
    "highest = {\n",
    "    interval: max(\n",
    "        [\n",
    "            d[\"inverse_error_quantiles\"][quintile]\n",
    "            for d in ods.get_metadata()[\"dataframe_stats\"][interval][\n",
    "                \"power_factors\"\n",
    "            ].values()\n",
    "        ]\n",
    "    )\n",
    "    for interval in ods.ohlc_intervals\n",
    "}\n",
    "\n",
    "lowest = {\n",
    "    interval: min(\n",
    "        [\n",
    "            d[\"inverse_error_quantiles\"][quintile]\n",
    "            for d in ods.get_metadata()[\"dataframe_stats\"][interval][\n",
    "                \"power_factors\"\n",
    "            ].values()\n",
    "        ]\n",
    "    )\n",
    "    for interval in ods.ohlc_intervals\n",
    "}\n",
    "\n",
    "print(f\"average 99% quintile of inverse error for the analyzed features: \\n{average}\")\n",
    "print(f\"highest 99% quintile of inverse error for the analyzed features: \\n{highest}\")\n",
    "print(f\"lowest 99% quintile of inverse error for the analyzed features: \\n{lowest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "193220f4-06b5-4708-b344-3f4a9eb72fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest inverse error for the analyzed features:\n",
      "\n",
      "{10: [('BBP_5_2.0', 1.0),\n",
      "      ('SQZ_20_2.0_20_1.5', 1.0),\n",
      "      ('STOCHRSIk_14_14_3_3', 1.0),\n",
      "      ('STOCHRSId_14_14_3_3', 0.8461538461538461),\n",
      "      ('MFI_14', 0.10816523075392998),\n",
      "      ('QS_10', 0.03896103896103894),\n",
      "      ('CFO_9', 0.005336000000000043),\n",
      "      ('NVI_1', 0.0020398571832649704),\n",
      "      ('BBB_5_2.0', 0.001877999999993626),\n",
      "      ('high_Z_30_1', 0.001444673852413607)],\n",
      " 20: [('SQZ_20_2.0_20_1.5', 1.0),\n",
      "      ('STOCHRSIk_14_14_3_3', 1.0),\n",
      "      ('STOCHRSId_14_14_3_3', 0.02570093457943925),\n",
      "      ('QS_10', 0.025641025641025664),\n",
      "      ('low_Z_30_1', 0.006513114328130283),\n",
      "      ('CFO_9', 0.0040239999999998844),\n",
      "      ('PGO_14', 0.001351068766824989),\n",
      "      ('BBB_5_2.0', 0.0012880000000014865),\n",
      "      ('high_Z_30_1', 0.0006254755957285614),\n",
      "      ('close_Z_30_1', 0.0005943526922971583)],\n",
      " 60: [('APO_12_26', 1.0),\n",
      "      ('QS_10', 1.0),\n",
      "      ('SQZ_20_2.0_20_1.5', 1.0),\n",
      "      ('STOCHRSIk_14_14_3_3', 0.5000000000000001),\n",
      "      ('MFI_14', 0.266088953394922),\n",
      "      ('STOCHRSId_14_14_3_3', 0.2134831460674159),\n",
      "      ('BIAS_SMA_26', 0.2),\n",
      "      ('high_Z_30_1', 0.005253801214697869),\n",
      "      ('PGO_14', 0.004699712336690413),\n",
      "      ('CFO_9', 0.0019600000000000086)]}\n"
     ]
    }
   ],
   "source": [
    "# the highest inverse error for the analyzed features\n",
    "highest_error = {\n",
    "    interval: sorted(\n",
    "        [\n",
    "            (k, v[\"max_inverse_error\"])\n",
    "            for k, v in ods.get_metadata()[\"dataframe_stats\"][interval][\n",
    "                \"power_factors\"\n",
    "            ].items()\n",
    "        ],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True,\n",
    "    )[:10]\n",
    "    for interval in ods.ohlc_intervals\n",
    "}\n",
    "print(\"highest inverse error for the analyzed features:\\n\")\n",
    "pprint(highest_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa8819-a95d-4067-8b82-c84e2970062c",
   "metadata": {},
   "source": [
    "In fact, only a few features generate large inverse errors. We save the transformation parameters in the metadata so that at a later stage, if we want, we can reject features with a high maximum error or a selected quintile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da773742-7f80-4b8b-8065-b4ea709995e2",
   "metadata": {},
   "source": [
    "We can save the prepared dataset by casting it only to numpy arrays or their compressed version, we can also save the entire class object as a pickle file or save the metadata to a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a8f2cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save whole instance\n",
    "ods.save(\"instance\")\n",
    "# save metadata\n",
    "ods.save_metadata(\"metadata\")\n",
    "# convert dataframes to numpy arrays and save it ~ 19MB, for this example\n",
    "ods.save_arrays(\"arrays\")\n",
    "# compressed variant ~ 12MB, for this example\n",
    "ods.save_compressed_arrays(\"arrays_compressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c096c7b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print dataset metadata\n",
    "pprint(ods.get_metadata())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
